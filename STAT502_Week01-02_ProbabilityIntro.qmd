# Introduction to Probability {#probability}

## Introduction

**Exercise: Birthdays**

Does anyone in this class have the same birthday?

What is the probability that 2 people in this class have the same birthday?

## Why Study Probability?

**Discussion** What decisions have you made today?  What factors did you take into account when making these decisions?

We live in an *Information Society* -- we must use the information presented to us to make intelligent decisions.

- Theory vs Reality
- ``All models are wrong, but some are useful'' (George Box, 1979)
- Deterministic Model vs Probabilistic  Models

### Deterministic Models

**Example 1** Lake Yeak Laom, Cambodia has a diameter of 0.72 km.\footnote{https://en.wikipedia.org/wiki/Lake\_Yeak\_Laom} What is the surface area of this lake?

```{r, echo=FALSE, fig.align='center', out.width='50%'}
knitr::include_graphics("figs/lake_yeak_laom.jpg")
```

### Probabilistic  Models

**Example 2** Suppose we are going to flip a coin. What will the outcome be?

We do not know for certain, but we can make a statement like, in the long run 1/2 the outcomes will be heads.

**Example 3** How many bottles of milk will Newsfeed Cafe use today?

We do not know for certain, but with some data we can make similar statements regarding the probability of particular outcomes.

### Applications of Probability

- Credit Risk Assessment -- should person X be given a loan?
- Maintenance Planning -- should a part be replaced routinely or when it fails?
- Population Modelling -- will a population become extinct?
- Disease Control -- what will the impact be of an outbreak of a pandemic disease?
- Customer Service -- how long will it take to serve a customer?
- Marketing -- will NetFlix customer X watch ``Designated Survivor'' given that they have watched ``Suits''?

Two uses of probability models:

- Use a given probabilistic model to gain insight.
- Determine whether a given probabilistic model is correct, based on data (Statistical Inference).

## Probability and randomness

Probability and randomness are very difficult to define without using circular logic, but easy to recognise.

In loose terms, probability is a measure of one's confidence that an event will occur.

*  What is the probability that it will rain tomorrow?
*  What is the probability that I will pass my statistics exam?
*  What is the probability that the train will be late?
*  What is the probability that a particular stock price will increase?
*  What is the probability that I will roll 3 sixes in a row?
*  What is the probability that I will get a job within six months of graduating?

Some of the questions above have well-defined and objective answers, some do not. Probability and randomness are subtle concepts and there are different interpretations of probability statements.

In this course, we will interpret probability as the long-run relative frequency of an event occurring.  For example, suppose a fair coin is flipped, then the probability of getting a head can be interpreted as the relative frequency of getting a heads over a very large number of trials. This will be close to one-half if the coin is truly fair.

(An alternative interpretation is subjective probability. This is a measure of an individual person's belief that an event will occur. We will not consider subjective probability in this course, but it is used in more advanced courses and is becoming the dominant methodology in certain scientific areas including climate change and astrophysics).

To start thinking about probability, we need some basic terminology.

## Random variables

A *variable* is a characteristic that changes or varies over time and/or for different individuals or objects under consideration

Examples:

* In a survey of university students, height, gender, income and age will all vary from person to person.
* The number obtained when rolling a die will vary with each roll.
* The price of ANZ shares will vary over time.
* whether the All blacks in or not in a game will change from match to match

A *random variable* (usually denoted with a capital letter, usually $X$) is a variable whose value is determined by the measuring the outcome of a random process. For example:

* $X$ = Number of defects on a randomly selected piece of furniture
* $X$ = Number of girls in a randomly selected family with 3 children
* $X$ = Number obtained when rolling a die
* $X$ = The height of a randomly selected AUT student
* $X$ = Time taken to respond to a randomly selected call to 111

(the above definition is somewhat circular---it uses randomness to define random---but giving a watertight definition requires several years of postgraduate study in pure mathematics even to state. We will use terminology somewhat loosely in this course).

## The language of probability

An *experiment* is any process that produces an observation or outcome. The set of all possible outcomes of an experiment is called the *sample space*.

If the experiment is tossing a coin, the sample space will be the set $\{\mathrm{head}, \mathrm{tail}\}$.  If it is throwing two dice, the sample space will ordered pairs $(a,b)$ where $a,b$ are members of the set $\{1,2,3,4,5,6\}$. Continuous variables are more problematic. We usually take the sample space to be the real numbers, or sometimes the
computer representation of the real numbers.

An *event* is a set of outcomes of the experiment, or a subset of the sample space. For example, if the experiment is tossing a six-sided die, then an event might be "obtaining an even number", that is, the result being in the set $\{2,4,6\}$. The simplest events are just
single points of the sample space.

### Example: three coloured balls in a bag

A box contains three ballsâ€”one red, one green, and one blue. Consider an experiment that consists of withdrawing a ball from the box, noting its colour, **replacing** it, and then withdrawing a second ball.

Here the sample space will be, with obvious notation,

RR, RG, RB, GR, GG, GB, BR, BG, BB

It might be easier to see what is going on by arranging the elements more logically:

RR, RG, RB

GR, GG, GB

BR, BG, BB


See how the elements of each row have the same first colour, and the elements of each column have the same colour. This makes it easier to check that you have written the sample space out correctly.

## Set theory

We are going to need a little set theory; see @fig-venn, which defines set intersection, set union, set complement and symmetric difference.  

```{r venn, fig.cap='Two sets, $A$ and $B$ on a Venn diagram together with red areas marking set union ($A\\cup B$), set intersection ($A\\cap B$), complement of A ($\\overline{A}$) and symmetric difference ($A\\Delta B$)', echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-venn
knitr::include_graphics("figs/intersection_union.pdf")
```

The *empty set* (written $\varnothing$) is the set with no elements. Thus the set of billionaire statistics lecturers is the empty set (but I'm working on it).

The *union* of $A$ and $B$ (written $A\cup B$) is the set of elements that are in either $A$ or in $B$, or in both.  Sometimes we say that the union is ``A or B''; but note that the English language is somewhat imprecise here: if I say to my daughter that she can have an
ice cream or a drink, I mean that she has to choose one or the other. She can't have both.

The *intersection* of $A$ and $B$ (written $A\cap B$) is the set of elements that are in both $A$ and in $B$.  Thus if $A$ is the set of all European cities, and $B$ is the set of all cities with a population of more than 3 million, then $A\cap B$ is the set of all European cities with a population of more than 3 million. If two sets have empty intersection (that is, $A\cap B=\varnothing$), we say that $A$ and $B$ are *disjoint*. Thus if $A$ is the set of billionaire philanthropist playboy vigilantes, and $B$ is the set of AUT statistics lecturers, then if a person is a member of set $A$ then he will not be a member of set $B$; and conversely. The sets are disjoint^[Yup, definitely disjoint.  Absolutely empty intersection there. Yes sir. No way these two sets could have any common members. Ask my butler.].

The *complement* of event $A$ (written $\overline{A}$; sometimes in books you will see $A'$ or $A^c$), is the set consisting of all elements which are not in $A$. For example, if we are sampling people in the street, we could define $A$ to be the event that the person is
a smoker. Then the complement, $\overline{A}$ would be that the person is not a smoker.

The *symmetric difference* of $A$ and $B$ (written $A\Delta B$) is $\left(A\cup B\right)\cap\overline{A\cap B}$.  It is the set of all in $A$ but not $B$, or in $B$ but not $A$.  This is what I mean when I say to my daughter that she may have an ice cream or a drink. I ought to say to her: "you've been a good girl today, you can choose any
member of the set comprising the symmetric difference of the sets $A$ (the set of having an ice-cream) and $B$ (the set of having a drink)". But note that this _obliges_ her to choose one or the other; she is not at liberty to decline my offer. She cannot have neither.

## Basic properties of probability

Here we will cover some basic facts about probability which are intuitively obvious but might be phrased in unfamiliar wording.

### Probability of the sample space

If we have a set $A$ then the number of elements in $A$ is written $n(A)$.  The *probability* of an event $A$ is denoted $p(A)$, or sometimes $Pr(A)$.  If events corresponding to single elements of $S$ are equally likely, we _define_ the probability of $A$ to be $\frac{n(A)}{n(S)}$. We have that $p(A)$ is between zero and 1, written $0\leq p(A)\leq 1$. The probability of the sample space $S$ is $\frac{n(S)}{n(S)}=1$; we would write $p(S) = 1$.

### Axioms of probability

To use the probability, we need to understand its assumptions or **axioms**.

**Axiom 1**: The probability that the outcome of the experiment is an outcome in $A$ is some number between 0 and 1.

$$
0\leq p(A)\leq 1
$$

**Axiom 2**: With probability of 1, the outcome will be a point in sample space $S$

$$
p(S)=1
$$

**Axiom 3**: For any sequence of mutually exclusive (disjoint) events, the probability of at least one of these events occurring is just the sum of their respective probabilities.

$$
p(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty} p(A_i)
$$

### Example (Ross, 2006, Example 3b) {.unnumbered}

If a die is rolled and we suppose that all six sides are equally likely to appear, then we would have 

$$
p(\{1\}) = p(\{2\}) = p(\{3\}) = ... = p(\{6\}) = \frac{1}{6}
$$
From Axiom 3 it would thus follow that the probability of rolling an even number would equal

$$
p(\{2,4,6\}) = p(\{2\}) + p(\{4\}) + p(\{6\}) = \frac{1}{2}.
$$

### Probability of set union and the principle of inclusion-exclusion

It is one of the axioms of probability that $p(A\cup B)\leq p(A)+p(B)$.  If two events $A$ and $B$ are disjoint, we have that $p(A\cup B) = p(A) +p(B)$.  If $A$ and $B$ are not disjoint, the situation is more complicated because we are double-counting elements
in $A\cap B$. We can also deduce that

$$
p(A\cup B)=p(A)+p(B)-p(A\cap B)
$$

which is known as the principle of inclusion-exclusion. If we have three sets the principle becomes more complicated still and we have

$$
p(A\cup B\cup C)=p(A)+p(B)+p(C)-p(A\cap B)-p(A\cap C)-p(B\cap C)+p(A\cap B\cap C)
$$

### Probability of set complement

The *complement* of event $A$, denoted $\overline{A}$ (sometimes in books you will see $A'$ or $A^c$), is the set consisting of all outcomes in the sample space which are not in $A$.  For example, if we are sampling people in the street, we could define $A$ to be the event
that the person is a smoker. Then the complement, $\overline{A}$ would be that the person is not a smoker. Because $A$ and $\overline{A}$ are disjoint, we have that

$$
p(A) + p(\overline{A})=p(A\cup\overline{A})=p(S)=1.
$$

Therefore

$$
p(\overline{A}) = 1 - p(A)
$$

In words, this says "the probability of the complement of $A$ is one minus the probability of $A$". Thus if $p(A)=0.35$ then the probability of being a non-smoker is 1-0.35, or 0,65. This makes sense intuitively.

## de Morgan's laws

de Morgan's laws are  the intuitively obvious set equations:

$$
\overline{A\cup B} = \overline{A}\cap\overline{B}
$$

and

$$
\overline{A\cap B} = \overline{A}\cup\overline{B}
$$

Take the first one as an example. This says that the probability of being "neither $A$ nor $B$" is equal to the probability of being "not $A$" and "not $B$".

## Conditional probability

Quite often, two events are interdependent in the sense that one conveys information about the other. For example if $A$ is the event "Sam has a glass of wine with his dinner" and $B$ is the event "Lesley has a glass of wine with her dinner", then knowing that $A$ has occurred changes the probability that $B$ occurs (because if we open a bottle of wine then we both have some).  We might have $p(A)=0.2$ and $p(B)=0.1$.

But if Lesley has wine, then it is almost certain that Sam will have some too. We express this in probability language using the concepts of conditional probability. We might ask what the probability that Sam has wine is, _given_ that Lesley has wine; this would be written $p(A|B)$ (and in words is "the probability of $A$ given $B$"). We might have $p(A|B)=0.9$. So the probability of me having wine, given that Lesley is having wine, is 90\%.

The conditional probability law states that

$$
p(A|B)=\frac{p(A\cap B)}{p(B)}
$$

(to verify this, look at the figure above).

Quite often, events do not have any connection with one another, and we have that $p(A) = p(A|B)$. In words, this means that the probability of $A$ occurring is equal to the probability of $A$ occurring, given that $B$ has occurred.  If this is the case we say that $A$ and $B$ are *independent*.  If two events are independent then we have

$$
p(A) = p(A|B)=\frac{p(A\cap B)}{p(B)}
$$

so we can deduce that independent events satisfy $p(A\cap B)=p(A)p(B)$ which can serve as an alternative definition of independence.

## Example: rolling two dice {.unnumbered}

Consider an experiment which consists of rolling two dice and observing the number on the uppermost face.  All of the outcomes of this experiment form the *sample space*:

\begin{align*}
S = \{	& (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), \\
	& (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), \\
	& (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), \\
	& (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), \\
	& (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), \\
	& (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)\}
\end{align*}

In the above, note how the order of the numbers matter: we are treating $(1,2)$ as a different result from $(2,1)$.   

Define events $A$ and $C$ as follows:

* event $A$: first number is 4 or 5
* event $B$: sum is 9 or greater (that is, if the throw is $(a,b)$ then $a+b\geq 9$)
* event $C$: first number is strictly less than the second number

Using the fact that $p(A) = n(A)/n(S)$ (and we know that $n(S)=6 \times 6=36$), we can see that

* $A=\left\{(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),(5,1),(5,2),(5,3),(5,4),(5,5),(5,6)\right\}$, so therefore $n(A) = 12$ and therefore $p(A)=n(A)/n(S)=12/36=1/3$.
* $B=\left\{(3,6),(4,5),(4,6),(5,4),(5,5),(5,6),(6,3),(6,4),(6,5),(6,6)\right\}$, so therefore $p(B) = 10/36=5/18$
* $C=\left\{(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(2,6),(3,4),(3,5),(3,6),(4,5),(4,6),(5,6)\right\}$, so therefore $p(C) = 14/36=7/18$

We can find probabilities for the intersection and union easily too. For example, if we wish to find $A\cap B$, then this is elements that are in set $A$ and set $B$. Thus we have to find throws that have the first number 4 or 5, and also whose sum is $\geq 9$.  This would be the set $\left\{(4,5),(4,6),(5,4),(5,5),(5,6)\right\}$, so $p(A\cap B)=5/36$.

### Example question 1 {.unnumbered}

Question: Find $p(A|B)$.

Answer: from the conditional probability formula, $p(A|B) =
\frac{p(A\cap B)}{p(B)} = \frac{5/36}{5/18}=\frac{1}{2}$.

### Example question 2 {.unnumbered}

Question:  find $p(A\cup B)$.

Answer: From the principle of inclusion-exclusion, we have

$$
p(A\cup B)=p(A)+p(B)-p(A\cap B) = 
\frac{1}{3}+\frac{5}{18}-\frac{5}{36}=\frac{17}{36}
$$

### Example question 3 {.unnumbered}

Question: Are events $A$ and $B$ independent?

Answer: independent events are characterised by $p(A\cap B)=p(A)p(B)$.

We have $p(A\cap B)=\frac{5}{36}$ but $p(A)p(B)=\frac{1}{3}\times\frac{5}{18}=\frac{5}{56}$ so the events are not independent.

### Example question 4 {.unnumbered}

Question: Calculate $p(A\cup B\cup C)$.

Answer: exercise for the reader.

## Example: grades

Suppose we have a class of students who take a statistics course and a physics course.  The pass rate for the statistics course is 60\% and the pass rate for the physics course is 70\%.  We also know that 50\% of students pass both physics and statistics.  This is shown in the form of a Venn diagram in @fig-statisticsphysics.

```{r statisticsphysics,fig.cap="Pass rates for statistics and physics.  The areas of the regions are accurately presented", echo=FALSE,message=FALSE,warning=FALSE}
#| label: fig-statisticsphysics
library("VennDiagram")
venn.plot <- draw.pairwise.venn(60, 70, 50, c("statistics", "physics"))
grid.draw(venn.plot)
```

### Example question 1 {.unnumbered}

Question:  what is the probability of passing either physics or statistics or both?

Answer: we seek $p(S\cup P)$.  By the principle of inclusion-exclusion, we have $p(S\cup P)=p(S)+p(S)=p(S\cap P)=0.6+0.7-0.5=0.8$. Students have an 80\% probability of passing at
least one paper.

### Example question 2 {.unnumbered}

Question:  what is the probability of failing both?

Answer: we seek $p(S'\cap P')$. By de Morgan, this is equal to $1-p(S\cup P)=1-0.8=0.2$.  Students have a 20\% probability of failing both.

### Example question 3 {.unnumbered}

Question:  A student is known to have passed physics. What is the probability that they pass statistics?

Answer: We seek $p(S|P)$.  By the formula for conditional probability, we have $p(S|P)=\frac{p(S\cap P)}{p(P)}=\frac{0.5}{0.7}=\frac{5}{7}$. Thus a student who passes physics has a probability of $5/7$ of passing statistics.

### Example question 4 {.unnumbered}

Question:  Are the events "passing statistics" and "passing physics" independent?

Answer: If $S$ and $P$ are independent, we have $p(S\cap P)=p(S)\times p(P)$.  We are told that $p(S\cap P)=0.5$, but $p(S)\times p(P)=0.6\times 0.7=0.42$ which is different.  The events are not dependent.

## Bayes' Theorem

Bayes' theorem, discovered by Reverend Thomas Bayes in 1763, makes use of the conditional probability and has proven useful in probabilistic modelling in many areas such as health science, biology, economics, finance, and astronomy. 

Let $A$ and $B$ are events and $p(B) \neq 0$. Then, Bayes' theorem states that

$$
p(A|B) = \frac{p(B|A)p(A)}{p(B)}
$$
where $p(A|B)$ is the posterior probability, $p(B|A)$ is a conditional probability and also called the likelihood, $p(A)$ is prior, and $p(b)$ is the marginal probability or marginal likelihood.

Basically, the posterior is an updated probability after the new information/data (likelihood) arrives, given the prior information. For example, we might think about the chance of a shower given the cloudy sky, based on prior knowledge and the fact that what happened the last time this similar situation occurred.

### Example (Ross, 2006, Example 3f) {.unnumbered}

At a certain stage of a criminal investigation the inspector in charge is 60 percent convinced of the guilt of a certain suspect. Suppose now that a new piece of evidence that shows the criminal has a certain characteristic (such as left-handedness, baldness, or brown hair) is uncovered. If 20 percent of the population possesses this characteristic, how certain of the guilt of the suspect should the inspector now be if it turns out that the suspect has this characteristic?

**Solution** Letting $G$ denote the event that the suspect is guilty and $C$ is the event that the suspect possesses the characteristic of the criminal, we have

\begin{align*} 
p(G|C) & =  \frac{p(G \cap C)}{p(C)} \\
 & =  \frac{p(C|G)p(G)}{p(C|G)p(G)+p(C|G')p(G')} \\
 & =  \frac{(1)(0.6))}{(1)(0.6)+(0.2)(0.4)} \\
 & \approx  0.882
\end{align*}

where we have supposed that the probability of the suspect having the characteristic is, in fact, innocent is 0.2, which is the proportion of the population possessing the characteristic.

## Tree diagrams

Tree diagrams is an application of the conditional probability and can be used to depict a series of events. They are particularly useful when computing probabilities associated with a series of events.

* Each node represents an event. 
* Branches are used to indicate connections between events.
* The probability of each event is written along the branch. 
* The probabilities on all branches stemming from the same event must sum to one.
* Joint probabilities can be found by multiplying along the branches and are often written at the end of a sequence of branches. 
* All of the joint probabilities should sum to one.


## Example of a tree diagram: testing for a disease {.unnumbered}

It is known that 0.5\% of the population have a particular disease. Of those that have the disease, 99\% will test positive for the disease. Of those that do not have the disease, 2\% test positive for the disease. We can represent this information as a tree diagram, as seen in @fig-tree. This defines event $D$ as a randomly chosen person has the disease, and Event $E$ as that person has a positive test.

```{r tree, out.width="6in",fig.cap='A tree diagram showing diagnosis of a disease', echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-tree
knitr::include_graphics("figs/tree_diagram.pdf")
```

Observe that this test is a "good" test: if you have the disease, the test probably detects this; and if you do not have the disease, the test probably says that you are clear.


### Example 1 {.unnumbered}

Question: what is the probability that a person chosen at random has the disease?

Answer: from the data, $P(D) =0.005 = 0.5\%$

### Example 2 {.unnumbered}

Question: what is the probability that a person chosen at random received a positive test?

Answer: There are two possibilities for receiving a positive test. Either you have the Disease and test positive, positive or you do not have the disease and test positive (this is known as a "false positive").

These two possibilities are $D \cap E$ and $D' \cap E$, which are disjoint.  Using the law of conditional probability we have $P(D \cap E) = P(E|D) P(D) = 0.00495$ and $P(D' \cap E) = P(E|D') P(D') = 0.0199$.

Thus $p(E)=p(D\cap E)+p(D' \cap E) = 0.00495+0.0199=0.02485$, or a little under $2.5\%$.

Note carefully that the majority of people with a positive test are actually free of the disease. Think about this and ask yourself what medical implications such analysis might have. Some diseases involve painful and potentially lethal treatment, and the treatment is
pointless unless you actually have the disease.

### Example 3 {.unnumbered}

Question: Are events D and E independent? Justify your answer.

Answer: $D$ and $E$ are independent if $P(E \cap D)=P(E)P(D)$. We have that $P(E \cap D) = 0.00495$ but $P(E)P(D) = 0.024855\times 0.005=0.00012$ approximately, so the events are not independent. Observe that the test is useful *because* events $E$ and $D$ are not independent.



